# Lecture 0 — 人工智能（Artificial Intelligence）

---

## 人工智能概述

人工智能（**AI, Artificial Intelligence**）指的是一系列让计算机表现出类似“智能”行为的技术。  
例如，AI 可以用来识别人脸（社交媒体上的照片）、打败世界象棋冠军、以及在你对着手机里的 Siri 或 Alexa 说话时，对语音进行理解和处理。

在本课程中，我们会探索一系列让 AI 成为可能的核心思想（下面这些都是 <mark>重点概念</mark>）：

0. **Search（搜索）**  
   为一个问题找到解法，例如导航应用为你从起点到终点找到最佳路线，或在游戏中为下一步行动做出决策。

1. **Knowledge（知识）**  
   表示信息，并从中进行推理与推断。

2. **Uncertainty（不确定性）**  
   使用概率来处理不确定事件。

3. **Optimization（优化）**  
   不仅仅找到“一个”可行解，而是寻找“更好”甚至是*最优*的解法。

4. **Learning（学习）**  
   基于数据和经验不断提升性能。比如邮件系统会根据以往的经验，从而区分垃圾邮件和正常邮件。

5. **Neural Networks（神经网络）**  
   一种受人脑结构启发的程序结构，能够高效地完成复杂任务。

6. **Language（语言）**  
   处理自然语言——也就是人类日常使用和理解的语言。

---

## 搜索（Search）

一个**搜索问题**通常包含一个 *初始状态（initial state）* 和一个 *目标状态（goal state）*，算法需要返回一条从前者到后者的**解（solution）**。  
在导航应用中，**agent（智能体）** 作为“思考的部分”，接收你当前的位置和目标位置作为输入，然后基于某种搜索算法，输出一条推荐路径。  
除了导航以外，还有很多其他形式的搜索问题，例如谜题（puzzles）或迷宫（mazes）。

以经典的 **15 拼图（15‑puzzle）** 为例，求解过程就需要使用搜索算法。为了形式化描述这类问题，我们引入如下几个关键概念（都是 <mark>核心名词</mark>）：

<p align="center">
  <img src="l_00_00.png" width="400" alt="Lecture 0 Search Puzzle">
</p>

- **Agent（智能体）**  
  感知环境并能对环境采取行动的实体。  
  在导航应用中，智能体可以看作是一辆车的抽象表示，它需要决定采取哪些动作才能到达目的地。

- **State（状态）**  
  智能体在环境中的一种具体配置。  
  例如，在 15‑puzzle 中，一个 *state* 就是棋盘上数字的一种排布方式。

  - **Initial State（初始状态）**  
    搜索算法开始时所在的状态。  
    在导航应用里，就是当前所在位置。

- **Actions（动作集合）**  
  在某个状态下可以执行的所有动作。  
  更严格地，可以把动作定义成一个函数：给定状态 *s*，`Actions(s)` 返回在状态 *s* 下可以执行的所有动作集合。  
  例如，在 15‑puzzle 中，当前状态下可以滑动空格周围的数字方块：  
  若空格在中间，则有 4 种滑动方式；若在边缘，则有 3 种；若在角落，则有 2 种。

- **Transition Model（状态转移模型）**  
  描述在任一状态中执行一个可用动作后，会到达哪一个新状态。  
  更正式地，转移模型可以表示为函数 `Result(s, a)`：给定当前状态 *s* 和动作 *a*，该函数返回执行该动作后得到的新状态。  
  例如，在 15‑puzzle 中，当我们在状态 *s* 下向某个方向滑动一块方块（动作 *a*），就会得到一个新的方块布局（新的状态）。

- **State Space（状态空间）**  
  从初始状态出发，通过一系列动作可以到达的所有状态的集合。  
  对于 15‑puzzle，状态空间包含所有可以到达的棋盘排布方式，总数是 \(16!/2\) 种。  
  状态空间通常可以被视为一个有向图：**节点（node）** 表示状态，**边（arrow）** 表示由动作引起的状态转移。

  <p align="center">
    <img src="l_00_01.png" width="500" alt="Lecture 0 Search Puzzle">
  </p>

- **Goal Test（目标检测）**  
  用来判断当前状态是否已经是目标状态的条件。  
  比如在导航应用中，目标检测就是：当前智能体位置（车所在的位置）是否已经到达目的地？  
  若是，则问题解决；若不是，就继续搜索。

- **Path Cost（路径代价）**  
  和某一条路径相关联的数值代价。  
  导航程序并不仅仅给出一条能到达终点的路径，还会试图 **最小化路径代价**，例如用时最短、距离最短或者收费最少。

---


## 解决搜索问题（Solving Search Problems）

- **Solution（解）**  
  从初始状态通往目标状态的一系列动作序列。

  - **Optimal Solution（最优解）**  
    在所有解中，**路径代价最小**的那个解，通常是我们真正想要的结果。

在搜索过程中，我们通常会把信息存储在一个叫做 **node（节点）** 的数据结构中。一个节点包含如下信息（这一结构在搜索算法中 <mark>极其重要</mark>）：

- 当前的 **state（状态）**  
- 生成当前节点的 **parent node（父节点）**  
- 从父节点到当前节点所执行的 **action（动作）**  
- 从初始状态到当前节点的 **path cost（路径代价）**

节点本身只是一种数据结构，但它非常有用：  
我们可以在每个节点的状态上进行 **goal test** 来检查它是否为目标；如果是，还可以比较不同节点的 **path cost**，从而选择*更优*甚至*最优*的解。  
由于每个节点都记录了父节点和对应动作，当我们找到某个目标节点后，可以从该节点一路回溯到初始节点，从而得到一条完整的动作序列，这个序列就是**解**。

需要注意的是：节点本身并不会“搜索”，它只负责“存信息”。真正负责“搜索”的，是管理这些节点的机制 —— **frontier（前沿或边界集）**。  
Frontier 一开始只包含初始状态，对应的“已探索集合（explored set）”为空。然后重复执行下面的步骤，直到找到解为止：

1. 如果 frontier 为空：  
   - *停止*。说明问题无解。
2. 从 frontier 中**移除一个节点**，作为当前要考虑的节点。
3. 如果该节点包含目标状态：  
   - 返回解，并 *停止*。  
   否则：  
   - **扩展（expand）** 当前节点：找到从该节点可以到达的所有新节点，并把它们加入 frontier。  
   - 将当前节点加入 **explored set（已探索集合）**。

---

## 深度优先搜索（Depth‑First Search, DFS）

在上面对 frontier 的描述中，我们还留下了一个问题：  
在第 2 步“从 frontier 中移除一个节点”时，**应该移除哪一个？**

这个选择将影响最终解的质量以及搜索所需时间。  
不同的选择策略就对应不同的搜索算法。其中有两个经典策略：

- 使用 **栈（stack）** —— 对应 **深度优先搜索（DFS）**  
- 使用 **队列（queue）** —— 对应 **广度优先搜索（BFS）**

这里先介绍 **深度优先搜索（DFS）**。

在 DFS 中，算法会沿着一个方向一直“走到尽头”，再回头换另一个方向。  
此时 frontier 被维护为一个 **栈（stack）** 结构。你需要记住的口号是：  
<mark>*last‑in first‑out（后进先出）*</mark>。  
换句话说，节点被不断加入 frontier，而在选择要扩展的节点时，总是优先取“最后加入”的那个。  
这就产生了一种“先走得尽量深，再回头”的搜索方式。

**生活中的类比：**  
假设你在找钥匙。如果采用 DFS 式思路，你先决定从裤子开始找，那么你会：

1. 把裤子的每一个口袋 **全部翻遍**；  
2. 确认裤子里再也不可能有钥匙以后，才会换到别的地方（比如桌子、抽屉等）继续找。

**DFS 的优点：**

- 在理想情况下，如果“碰巧”一开始就沿着正确路径前进，那么 DFS 可能是**最快**的算法。

**DFS 的缺点：**

- 找到的解 **不一定是最优解**。  
- 在最坏情况下，它可能需要枚举 **所有可能的路径** 之后才找到解，耗时最长。

**代码示例：DFS 中从 frontier 移除节点**

```python
# 定义一个函数，从 frontier 中取出一个节点并返回
def remove(self):
    # 如果 frontier 为空，则说明无解，终止搜索
    if self.empty():
        raise Exception("empty frontier")
    else:
        # 取出列表中最后一个元素（最新加入的节点）
        node = self.frontier[-1]
        # 丢弃最后一个元素，相当于从栈顶弹出
        self.frontier = self.frontier[:-1]
        return node
```

---

## 广度优先搜索（Breadth‑First Search, BFS）

与 DFS 相对的是 **广度优先搜索（BFS）**。

在 BFS 中，算法会**同时沿多个方向向前推进**：先在各个方向上走一步，再在各个方向上走第二步，以此类推。  
此时 frontier 被维护为一个 **队列（queue）** 结构。你需要记住的口号是：  
<mark>*first‑in first‑out（先进先出）*</mark>。  
所有新加入的节点会排成一列，算法总是优先取出“最早加入”的那个节点进行扩展。  
因此，BFS 会在所有可能方向上先走一层，再走下一层。

**生活类比：**  
仍然是找钥匙的例子。如果采用 BFS 思路，你可能会这样做：

1. 先检查裤子右口袋；  
2. 接着检查一个抽屉；  
3. 再看看桌面；  
4. ……把所有可能的地方都先检查一遍；  
5. 最后才会回到裤子，检查左口袋或其他口袋。

**BFS 的优点：**

- 在所有步长（单步代价）相同的情况下，BFS **一定能找到最优解**。

**BFS 的缺点：**

- 运行时间通常比“理论最短时间”要长。  
- 在最坏情况下，它同样可能需要很长时间，甚至是**最长时间**。

**代码示例：BFS 中从 frontier 移除节点**

```python
# 定义一个函数，从 frontier 中取出一个节点并返回
def remove(self):
    # 如果 frontier 为空，则说明无解，终止搜索
    if self.empty():
        raise Exception("empty frontier")
    else:
        # 取出列表中的第一个元素（最早加入的节点）
        node = self.frontier[0]
        # 丢弃第一个元素，相当于从队列头部出队
        self.frontier = self.frontier[1:]
        return node
```

---

## 贪心最佳优先搜索（Greedy Best‑First Search）

DFS 和 BFS 都属于 **uninformed search（无信息搜索）**，也就是说：  
它们在搜索过程中并不会利用任何“额外知识”，所有信息都来自于自身的探索。

但在很多实际问题中，我们确实可以获得一些“额外知识”。  
比如，人类在迷宫的岔路口时，通常能直观地判断哪一个方向“更接近出口”。  
如果把这种“直觉”形式化，引入到搜索算法中，就得到所谓的 **informed search（有信息搜索）**。

**Greedy Best‑First Search（贪心最佳优先搜索）** 的思想是：  
每一步都选择当前看起来“离目标最近”的那个节点进行扩展，这个“最近”由一个 **启发式函数（heuristic） \(h(n)\)** 来度量。  
顾名思义，启发式函数是对“距离目标还有多远”的一种 **估计（estimate）**。

在迷宫问题中，一个常用的启发式就是 **Manhattan distance（曼哈顿距离）**：  
它忽略墙壁，只计算在网格中从当前位置到目标位置，**只允许上下左右移动** 时所需的步数。  
在坐标平面上，这可以通过当前坐标 \((x, y)\) 与目标坐标的差值很容易算出。

<mark>启发式（Heuristic）= 对“到目标的距离”的有根据的猜测。</mark>

需要注意的是：任何启发式都有可能出错，从而让算法走上一条比原本更慢的路径。  
在某些情况下，一个 **无信息搜索算法** 甚至可能更快地找到更好的解；不过从整体上看，有信息搜索更有可能更快地找到高质量解。

---

## A\* 搜索（A* Search）

在贪心最佳优先搜索的基础上，**A\*** 搜索进一步改进了策略。  
它同时考虑：

- \(h(n)\)：从当前节点到目标节点的 **估计代价**；
- \(g(n)\)：从起点到当前节点已经累积的 **真实代价**。

于是定义：

\[
f(n) = g(n) + h(n)
\]

作为**从起点经过当前节点到达目标的估计总代价**。  
在搜索过程中，一旦当前路径的 \(f(n)\) 超过了某条备选路径的估计代价，算法就会放弃当前路径，转而回到那条“更有希望”的路径上，从而避免在明显低效的路径上浪费太多时间。

但要记住，A\* 仍然依赖于启发式函数，因此算法的好坏在很大程度上取决于所选的启发式。  
在某些情形下，它甚至可能不如贪心搜索或无信息搜索高效。

为了让 A\* 搜索在代价一致的情形下 **保证最优性**，启发式函数 \(h(n)\) 需要满足：

1. **Admissible（可采纳）**：从不 **高估** 真正的代价；  
2. **Consistent / Monotonic（一致性）**：对于任意节点 \(n\) 及其后继 \(n'\)，带有步长代价 \(c\)，都满足  

   \[
   h(n) \leq h(n') + c.
   \]

   也就是说：从 \(n\) 走一步到 \(n'\) 再到目标的估计代价，不会比直接从 \(n\) 到目标的估计代价更小。

---

## 对抗式搜索（Adversarial Search）

前面讨论的搜索算法主要针对一个 **静态环境**：只要找到一条合适路径即可。  
而在 **对抗式搜索** 中，环境中存在一个“对手”，它会尝试实现与你相反的目标。

在很多游戏中（例如 Tic‑Tac‑Toe 井字棋、国际象棋等），AI 就需要使用对抗式搜索来做决策。

---

## Minimax 算法

在对抗式搜索中，一个经典算法是 **Minimax**。  
它将博弈的结果用数值来表示：

- 一方的胜利记为 **+1**；  
- 另一方的胜利记为 **−1**；  
- 平局通常记为 **0**。

于是：

- **Max 玩家（maximizing player）** 希望得到尽可能大的值；  
- **Min 玩家（minimizing player）** 希望得到尽可能小的值。

### 用 Minimax 表示 Tic‑Tac‑Toe（井字棋） AI

我们可以用如下方式形式化 Tic‑Tac‑Toe（以下术语都 <mark>非常重要</mark>）：

- **S₀**：初始状态（空的 3×3 棋盘）。  
- `Player(s)`：给定状态 *s*，返回当前该轮到哪一个玩家下子（X 或 O）。  
- `Actions(s)`：给定状态 *s*，返回该状态下所有合法动作（棋盘上所有空位）。  
- `Result(s, a)`：给定状态 *s* 和动作 *a*，返回执行该动作后得到的新状态（即在某个格子上落子后的棋盘）。  
- `Terminal(s)`：给定状态 *s*，判断游戏是否结束（有人赢了或平局）。若结束则返回 `True`，否则返回 `False`。  
- `Utility(s)`：给定一个终止状态 *s*，返回该状态的效用值：**−1**、**0** 或 **+1**。

### Minimax 的工作方式

Minimax 会**递归地**枚举从当前状态出发到所有可能终止状态的全部博弈过程。  
每一个终止状态都会被赋值为 −1、0 或 +1。

由于算法知道“现在轮到谁下”，它就能判断：

- 若当前轮到 Max 玩家，则会选择能让 **最终结果值尽可能大** 的动作；  
- 若轮到 Min 玩家，则会选择能让 **最终结果值尽可能小** 的动作。

这可以理解为一种不断在 “max” 和 “min” 之间交替的推理过程：

- Max 玩家会问：  
  *“如果我在当前状态执行动作 a，会到达一个新的状态。接下来 Min 玩家若也采取最优策略，这条路线最终能得到的值是多少？”*
- 为了回答这个问题，Max 玩家需要在脑海中模拟 Min 玩家：  
  *“如果我是 Min 玩家，在这个状态下我会如何选择动作，以让最终的值尽可能小？”*

如此往复，直到到达所有终止状态。  
最终，Max 玩家会为所有可能动作对应的后续状态计算出一个数值，然后选择**数值最大**的那个动作。

### Minimax 伪代码

直观上，可以认为 Max 玩家在评估 **所有未来可能局面的取值**。

伪代码如下（用 Python 风格表示）：

- **给定状态 `s`：**
  - Max 玩家选择 `Actions(s)` 中使得 `Min-Value(Result(s, a))` 最大的动作 `a`。  
  - Min 玩家选择 `Actions(s)` 中使得 `Max-Value(Result(s, a))` 最小的动作 `a`。

- **函数 `Max-Value(state)`：**

```python
def max_value(state):
    v = float("-inf")
    if Terminal(state):
        return Utility(state)
    for action in Actions(state):
        v = max(v, min_value(Result(state, action)))
    return v
```

- **函数 `Min-Value(state)`：**

```python
def min_value(state):
    v = float("inf")
    if Terminal(state):
        return Utility(state)
    for action in Actions(state):
        v = min(v, max_value(Result(state, action)))
    return v
```

---

## Alpha–Beta 剪枝（Alpha–Beta Pruning）

**Alpha–Beta Pruning** 是对 Minimax 的一种重要优化。  
它的思想是：**提前排除那些不可能影响最终决策的分支**，从而减少需要递归计算的节点数。

在搜索树中，当我们为某个动作已经计算出一个值之后，如果在探索某个新的动作分支时，**已经可以确定该分支不可能优于当前已知的最好结果**，就没有必要再继续深入这个分支，可以直接“剪掉”（prune）。

举一个示例来说明：

- 假设当前轮到 Max 玩家行动，他知道下一步轮到 Min 玩家会试图让结果尽可能低。  
- Max 玩家有三个可选动作，其中第一个动作在完成所有推演后，得到的值为 **4**。  
- 现在开始评估第二个动作：  
  - 为此，Max 玩家需要考虑：如果自己选择第二个动作，接着 Min 玩家有哪些应对方式，并且 Min 会从中选择**值最小**的那一个。  
- 在这个过程中，还没有把第二个动作所有后续分支都算完，Max 玩家就已经发现某个分支的值为 **3**。  
  - 如果后面有一个分支的值为 **10**，Min 玩家也会在 `3` 和 `10` 中选择 **3**；  
  - 如果后面出现一个分支的值为 **−10**，Min 玩家会选择 **−10**，结果对 Max 玩家更糟糕。  

因此，无论尚未计算的那些分支值是多少，都不会让这条路径比“已知值为 4 的第一条路径”更好，  
这时就可以直接停止对当前动作剩余分支的计算 —— 这就是 **Alpha–Beta 剪枝**。

<mark>要点：只要可以确定一个分支“不可能改写当前最优结果”，就可以安全剪枝。</mark>

---

## 限深 Minimax（Depth‑Limited Minimax）

理论上，Tic‑Tac‑Toe 的所有可能对局数约为 **255,168**，而国际象棋的可能对局数量则在 **10¹²⁰** 量级。  
按原始 Minimax 的定义，要从某个状态出发枚举**所有**可能对局直到终局，  
对于 Tic‑Tac‑Toe 这还可以接受，但对国际象棋就几乎不可能做到。

**Depth‑Limited Minimax（限深 Minimax）** 的做法是：  
只向前推演一个 **预先设定的最大深度**，即在没有真正到达终局之前就停止递归。

这样做的代价是：我们再也不能为每个动作计算出准确的最终胜负值。  
为了解决这个问题，我们引入一个 **评估函数（evaluation function）**，用来 **估计** 从当前状态出发的局面好坏。

例如在国际象棋中，一个评估函数可以这样工作：

1. 输入当前棋盘的整体布局（双方所有棋子的位置与类型）；  
2. 根据一些规则和经验，对这个局面的“形势”进行**打分**，  
   比如多一个棋子、控中心格子更好、防守更稳固等；  
3. 返回一个数值：  
   - 若对某一方更有利，则给出一个 **正数**；  
   - 若对另一方更有利，则给出一个 **负数**。

Minimax 算法就利用这些估计值来在有限深度下选择动作。  
评估函数越准确，整个 **Depth‑Limited Minimax** 策略就越接近真正的最优决策。

---
